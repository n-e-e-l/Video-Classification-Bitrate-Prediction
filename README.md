# Video-Classification-Bitrate-Prediction



The Abundance of streaming video applications has increased the competition apart from the content of the video, the quality of the video also matters. The original video would require huge bandwidth to stream on a device assuming the device can support such a high-quality video. This is where the encoding of the video comes in handy. Encoding of video means compressing the video to take less space (this can be done by removing unimportant information) and less data usage required to stream. When the video is streamed by the end-user, it is an approximation of the original video, which is usually good enough for the user to consume the content without noticing the difference. For streaming video online, the original videos are pre-encoded according to a strategy and when the user plays the video, the best-encoded video is streamed. The best-encoded video here refers to a video with maximum quality while avoiding buffering. One of the important factors that influence the quality of the streaming video is bitrate. 

Encoding strategy plays a vital role in determining the quality of the video which will be played for end consumers. Consider a simple scenario: Original video ‘A’ has 3 pre-encoded variants to stream- 480p, 720p
and 1080p out of these 3 options which one is to be played. This decision seems a no-brainer if you just want the best quality possible but in reality, bitrate is one of the important factors that influence the decision. Let’s further say bitrate is constant 1Mb/s so streaming 480p would be smooth but it won’t be the best quality available and streaming 1080p could include artifacts while streaming. This is further complicated when other factors such as not a constant bitrate and
device capability are considered. Every consumer needs bang for the buck and in this case value for their internet data being consumed to stream a video.

The encoding strategy to solve this very issue Initially used by Netflix Inc. used “Best Recipe
for All” \cite{aaron_anne_per-title_nodate} in this approach, almost all the video available on Netflix was used and various combinations were tested visually side by side. The trade-off between the quality of the video and bitrate was settled to come up with an encoding ladder that applied to all the videos. 
This approach worked but with constraints, One of the notable issues observed was that the encoding ladder varied considerably on the content of the video. It was observed that scenes that had a lot of changes required more bitrate to stream for example action scenes required more bitrate to achieve a particular resolution than animated movies which could achieve the same resolution in considerably low bitrate. 

Consider the sample Best Recipe for All encoding ladder below:

\begin{center}
\begin{tabular}{ll} 
\hline
Bitrate (kbps)  & Resolution\\ 
\hline
200 & 320 X 240 \\ 
\hline
550 & 512 X 384  \\ 
\hline
700 & 640 X 480 \\ 
\hline
1240 & 720 X 480  \\ 
\hline
2300 & 1280 X 720  \\ 
\hline

\end{tabular}
\caption{Table 1: Sample Encoding Ladder}
\end{center}

To get the resolution of 1280 X 720 bitrate(kbps) of 2300 is required now this scenario works for normal movies but for an animated movie this combination is not ideal because to get 1280 X 720
one would require only 700 kbps as opposed to 2300 kbps. The reason one observes this difference is due to the complexity of the video. A animated video only composes of flat frames with no grain noise of camera and minimal motion between consecutive frames where as a intense action sequence in a movie is very complex due to temporal motion and spatial texture. This meant that each title required its own specific encoding ladder. For each video practical constraints are considered to make an optimal per title encoding ladder, such as a finite set of resolutions and bitrates. Out of these finite sets, bitrate-resolution pairs that are close to the convex hull make up the optimal encoding ladder. This approach, however, comes with disadvantages: high use of storage as a result of hundreds of encodes that are tested and long computational time to visually compare the quality of various bitrate.

\section{Motivation:}
The impact of video quality on the end consumer’s quality of experience is still under research. It is difficult to quantify the impact of video quality due to human perception and judgment. The effects of quality changes are dependent on memory effects; people are less positive about bitrates that are increasing over time than they are negative to bitrates decreasing, as mentioned in \cite{pinson_comparing_2003}. Viewer quality of the experience was shown to be affected negatively in case of abrupt resolution changes \cite{grafl_representation_2013, egger_impact_2014}, or when the frequency of resolution switches is found to be annoying. 

Encoding of video is not limited to application based Streaming services. Streaming platforms have their own specific encoding strategy in place and a recommendation of settings for the uploaded video: YouTube \cite{noauthor_youtube_nodate} and Dailymotion \cite{noauthor_dailymotion_nodate}. The encoding of video is more important to streaming services as there is no oversight of quality of video being uploaded. Even though a set of recommended encoding for a video is provided it is left on the up-loader's choice. So any video when uploaded on a streaming platform is encoded according to the implemented strategy.
Considering the significant impact of video quality currently, Netflix’s proposed per-title encoding strategy is better than previous approaches as shown \cite{aaron_anne_per-title_nodate} but it requires a significant amount of testing and human intervention before an optimal per title encoding ladder can be derived for a particular title. This means that streaming platforms or anyone with an abundance of video will find it difficult to implement this groundbreaking encoding strategy as there is just too much video at varying quality being uploaded each minute. 

To tackle this very problem a machine learning-based classification of video from the context of the per-title encoding and a prediction of a bitrate of an encoded video is proposed. The classification of video into “animated” and “non-animated” would help in the fast decision of the encoding ladder’s bitrate range and bitrate prediction would help in making the encoding ladder. 
\section{Research Question:}
In this thesis, the following research questions will be addressed.
\begin{itemize}
  \item How to build a feature-based video classification model and prediction of bitrate for machine learning-based per title encoding?
  \item How does video classification using feature-based approaches and video classification using Deep Learning-based approaches compare for a binary video classification problem?
    \item What are the features that can help in predicting the bitrate of an encoded video in per title encoding ladder?
\end{itemize}

\section{Scientific Perspective And State Of The Art:}
In general, a video consists of three main aspects: audio, text or speech, and visual. For building a feature based machine learning model, these aspects are analyzed and used in feature set. Jasinschi et. al \cite{jasinschi_automatic_2001} segmented the video removing the commercial duration using visual features and then extracted audio from the segmented video portion to produce probability values for six categories: noise, speech, music, speech + noise, speech + speech, and speech + music and used these as the audio based feature vector. Audio features along with captioning as text-based features are used to classify videos. In general, the audio feature can be divided into two major parts: time-based audio features (like zero-crossing rate, amplitude envelope, root mean squared energy), and frequency-based audio features (like band energy ratio, Mel frequency, spectral centroid). Text-based feature analysis of closed captions (subtitle: text that are displayed over the video) is one of the most widely used approaches for video classification \cite{peng_wang_hybrid_2003,brezeale_automatic_2008}. Brezeale et. al \cite{brezeale_automatic_2008} used text feature based based on the availability and visual based color histograms RGB value for classification. Wang et. al \cite{peng_wang_hybrid_2003} had a multi-modal approach for video classification where features used ranged from audio features to text-based features extracted from the video. In Zhu et. all. \cite{weiyu_zhu_automatic_2001} the idea is to capture text from captions using Optical character recognition  (OCR). In the absence of the captions, the text is captured from the frames as well. Once a text based feature set is obtained from the video Natural Language Processing(NLP) steps are followed to convert the characters into Term Frequency. Inverse Document Frequency(TF.IDF). Visual features can be divided broadly into frame extracted features and Spatio-temporal features. Frame-based visual features include analyzing the color histograms of frames, using a frame-only approach for classification, and analyzing hierarchical matching pursuit for feature generation from an frame. A. Karpathy et. al \cite{karpathy_large-scale_2014} proposed a deep learning-based approach for the video classification with 1M YouTube data-set consisting of 473 classes. Firstly, to capture temporal dependencies present in the video different strategies used to combine frames of a video and then passed as input to the convolutional neural network (CNN). Based on this four techniques were proposed: Single fusion, Late fusion, Early fusion, and Slow fusion. Further, Multiresolution CNN was analyzed the aim was to make the learning faster. So the video was divided into 2 chunks and the frame from each of the chunks was fed into two parallel streams of the CNN network and then further combined. 
To predict the bit rate of an encoded video Sarah Wassermann et. al \cite{wassermann_let_2019} extracted 208 features ranging from time-based features, network features, and video features. A feature set for 15,000 YouTube videos was derived and a K- Nearest Neighbor model was trained. Adaptation strategy Video Bit-rate Prediction Model (VBPM) proposed by A. Lekharu et. al \cite{lekharu_qoe_2018} which is a Long Short Term Memory (LSTM) based deep learning model trained on 500 seconds clips. The bitrate prediction is used to decide the next segment of the clip so that the quality of the experience remains the same. In one of the early attempts, V. Menkovski et. al \cite{v_Menkovski} analyzed bitstream, Framerate, Bitrate, and video to propose a decision tree model for Quality of experience at runtime.  
D. Kumar et. al \cite{kumar_double_2018} propose an algorithm based to predict resolution and bitrate using reinforcement learning for the purpose of improving the quality of the video streamed online. 

\section{Research Methodology: }
To approach the implementation CRISP-DM general framework will be followed \cite{p._chapman_crisp-dm_2000}. Business Understanding, In this phase  the limitation of the per-title-encoding were understood. As the categories of video are predefined as animated and non-animated, the approach for this video classification problem will be a binary classification with labels: Animated and Non-Animated. The significant difference between the two means that the corresponding optimal per title encoding ladder will vary as well. The specific research questions help in understanding the need for data analysis. Data Understanding is the next phase here video data will be collected. For the purpose of video classification, the data will be labeled animated and non-animated. The same video data will be used for the prediction of the bitrate. Once the dataset is built exploration of individual data that is video will be done to understand the distribution of basic metadata like runtime, resolution, and size. In the next phase of the data preparation audio, text, and visual features from the video would be extracted. Based on the extracted features, correlation of the extracted data with the target labels will be analyzed to derive a final set of features. In the modeling phase, the final set of features will then be used to build a classification model for the video classification aspect and a regression model for the prediction of the bitrate. 

\section{Evaluation: }
The video classification task is a classification task, where accuracy will be one of the measurements in providing insight as to how accurately a model can predict the output. F1 measure is the harmonic mean of the model’s precision and recall will be another evaluation metric. Predicting the bitrate value of a video is a regression task, Mean Squared Error (MSE) will be used as one of the evaluation metrics. MSE calculates the average of the squared difference between values predicted by the model and the target values.
